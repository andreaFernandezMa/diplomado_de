{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "t7pKJ5yqhObG",
        "yiZj80P1hoJT",
        "yRkP-nWrsGa4",
        "0JbbtdwasN8b",
        "iSuDaBr3sbxi",
        "kyiRbf8NsqXi",
        "MjA4nvKUtY6E",
        "gEFLNvPVts6K",
        "zplgUDe0oTbV",
        "z_q3FcMcofUh",
        "ACgiU5Yioi2Z"
      ],
      "mount_file_id": "16HC3K68d3l4FujWnJsxVHq1QtLZhc8JG",
      "authorship_tag": "ABX9TyN+Ik+Unu+1SFuTTIlpV+Ly",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreaFernandezMa/diplomado_de/blob/main/Tareas_Andrea_Fernandez.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pyspark Install"
      ],
      "metadata": {
        "id": "t7pKJ5yqhObG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqpG-9rkgxov",
        "outputId": "dd1e2a48-bc20-4e7f-8107-b24f7169f6af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rIgn:1 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Connecting to security.\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:13 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [92.1 kB]\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,108 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,540 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,318 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,965 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,080 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,397 kB]\n",
            "Fetched 13.8 MB in 4s (3,557 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "# descomentar esta linea si falla al instalar openjdk\n",
        "!apt-get update"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Instalacion jdk y descargando spark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "UMu3O2DohNoF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instalación findspark\n",
        "!pip install -q findspark\n",
        "#Seteo de variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "8XZ0zS6MhiTU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instalar paquete pyspark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAO2RbLGhltj",
        "outputId": "8a7f1acd-69fb-454d-85ae-f133c2c74deb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 49 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 47.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=f286c831edc2a2f6f9e9b04ee1e6bcf89033aafebeacb9c374bfaf681a61d015\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Crear SparkSession"
      ],
      "metadata": {
        "id": "yiZj80P1hoJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Iniciar SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "3IMqaX-GhsyM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Otras Librerias\n",
        "from pyspark.sql.functions import *\n"
      ],
      "metadata": {
        "id": "D5Qwc2VMrf7b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Conectar google drive con Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5MgPY8OhzIm",
        "outputId": "a82c5ff6-703a-4b50-8204-8f1700facb8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TAREA 1"
      ],
      "metadata": {
        "id": "LEPiPxoKFGdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 1 (30%)\n",
        "\n",
        "Utilizando el dataset de [Walmart Stock](https://drive.google.com/file/d/154-RydBa4MOaubvLsiiGtc2HJW1alCkv/view?usp=sharing) realice lo siguiente:\n",
        "\n",
        "*   Inicie una sesión de Spark\n",
        "*   Cargue el archivo de Walmart Stock csv desde su cuenta Drive a un dataframe, permita que spark infiera el schema y tipos de datos\n",
        "*   imprima el schema, si no es correcto, cree usted el schema y cargue de nuevo el archivo.\n",
        "*   Imprima las primeras 5 filas\n",
        "*   Relice un *Describe* de la data\n",
        "*   Cree un nuevo Dataframe con una columna llamada **HV Ratio** el cual tenga la tasa entre **High** versus **Volume** \n",
        "*   Determine la media de la columna **Close**\n",
        "*   Determine el máximo valor de **High** por año.\n",
        "*   Determine el promedio de **Close** por cada mes."
      ],
      "metadata": {
        "id": "XcikMIv4h_BF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Inicie una sesión de Spark"
      ],
      "metadata": {
        "id": "yRkP-nWrsGa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Inicie una sesión de Spark\n",
        "#comprobar que la sesion de spark esta ok\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "gwWQ4zoNj4hy",
        "outputId": "b0fa547d-74ba-4dc6-9f82-4cbc9285db42"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f2cdb905410>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://46a8654c03c1:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Cargue el archivo de Walmart Stock csv\n",
        "\n",
        "Se creo el schema debido a que los datos eraan leidos como string"
      ],
      "metadata": {
        "id": "0JbbtdwasN8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import (\n",
        "    DateType,\n",
        "    FloatType,\n",
        "    IntegerType,\n",
        "    StructType,\n",
        "    StructField, \n",
        "    StringType, \n",
        "    IntegerType\n",
        ")"
      ],
      "metadata": {
        "id": "xa62kyvXTfWZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creacion de schema\n",
        "walmart_schema = StructType(\n",
        "    [\n",
        "        StructField('Date', DateType(), False),\n",
        "        StructField('Open',FloatType(), False),\n",
        "        StructField('High',FloatType(), False),\n",
        "        StructField('Low',FloatType(), False),\n",
        "        StructField('Close',FloatType(), False),\n",
        "        StructField('Volume',IntegerType(), False),\n",
        "        StructField('Adj Close',FloatType(), False),\n",
        "   ]\n",
        ")"
      ],
      "metadata": {
        "id": "HWxJ_RAUSDGT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargue el archivo de Walmart Stock csv desde su cuenta Drive a un dataframe, permita que spark infiera el schema y tipos de datos\n",
        "walmart = '/content/tarea1.1/walmart_stock.csv'\n",
        "#df_walmart = spark.read.csv(walmart, header=True, inferSchema=True) \n",
        "df_walmart = spark.read.csv(walmart, header=True, schema=walmart_schema) "
      ],
      "metadata": {
        "id": "obdFFBgki-BO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####imprima el schema\n",
        "Se importaron los formatos que necesitaremos para poder desarrollar el ejercicio"
      ],
      "metadata": {
        "id": "iSuDaBr3sbxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imprima el schema, si no es correcto, cree usted el schema y cargue de nuevo el archivo.\n",
        "df_walmart.printSchema() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11rIIw5nkhvy",
        "outputId": "7135a3fd-1822-43ba-e04c-19d229e085d2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Open: float (nullable = true)\n",
            " |-- High: float (nullable = true)\n",
            " |-- Low: float (nullable = true)\n",
            " |-- Close: float (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Adj Close: float (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Imprima las primeras 5 filas"
      ],
      "metadata": {
        "id": "kyiRbf8NsqXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imprima las primeras 5 filas\n",
        "df_walmart.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMjdPK-9lpvN",
        "outputId": "646f5c04-347c-4e86-a9c9-636792a8b298"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+-----+-----+--------+---------+\n",
            "|      Date| Open| High|  Low|Close|  Volume|Adj Close|\n",
            "+----------+-----+-----+-----+-----+--------+---------+\n",
            "|2012-01-03|59.97|61.06|59.87|60.33|12668800|52.619236|\n",
            "|2012-01-04|60.21|60.35|59.47|59.71| 9593300|52.078476|\n",
            "|2012-01-05|59.35|59.62|58.37|59.42|12768200| 51.82554|\n",
            "|2012-01-06|59.42|59.45|58.87| 59.0| 8069400| 51.45922|\n",
            "|2012-01-09|59.03|59.55|58.92|59.18| 6679300|51.616215|\n",
            "+----------+-----+-----+-----+-----+--------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Relice un Describe de la data"
      ],
      "metadata": {
        "id": "MjA4nvKUtY6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Relice un Describe de la data\n",
        "df_walmart.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSi6mFPql9az",
        "outputId": "354e5933-50d7-46a4-ad30-10a88553bc2f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, Open: string, High: string, Low: string, Close: string, Volume: string, Adj Close: string]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Cree un nuevo Dataframe \n",
        "Con una columna llamada HV Ratio el cual tenga la tasa entre High versus Volume"
      ],
      "metadata": {
        "id": "gEFLNvPVts6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cree un nuevo Dataframe con una columna llamada HV Ratio el cual tenga la tasa entre High versus Volume\n",
        "df_walmart2 = df_walmart.withColumn('HV', col('High')/col('Volume'))"
      ],
      "metadata": {
        "id": "ksNP9rErlGkc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_walmart2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsq3v2ammwas",
        "outputId": "bdf93c3e-b868-47ba-8e25-a4d5d4393c74"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+-----+-----+--------+---------+--------------------+\n",
            "|      Date| Open| High|  Low|Close|  Volume|Adj Close|                  HV|\n",
            "+----------+-----+-----+-----+-----+--------+---------+--------------------+\n",
            "|2012-01-03|59.97|61.06|59.87|60.33|12668800|52.619236|4.819714682786927E-6|\n",
            "|2012-01-04|60.21|60.35|59.47|59.71| 9593300|52.078476|6.290848662516662E-6|\n",
            "|2012-01-05|59.35|59.62|58.37|59.42|12768200| 51.82554| 4.66941298944916E-6|\n",
            "|2012-01-06|59.42|59.45|58.87| 59.0| 8069400| 51.45922| 7.36733843444859E-6|\n",
            "|2012-01-09|59.03|59.55|58.92|59.18| 6679300|51.616215|8.915604814435727E-6|\n",
            "|2012-01-10|59.43|59.71|58.98|59.04| 6907300| 51.49411|8.644477449144044E-6|\n",
            "|2012-01-11|59.06|59.53|59.04| 59.4| 6365600|51.808098|9.351828386844425E-6|\n",
            "|2012-01-12|59.79| 60.0| 59.4| 59.5| 7236400|51.895317| 8.29141562102703E-6|\n",
            "|2012-01-13|59.18|59.61|59.01|59.54| 7729300|51.930202|7.712212051589609E-6|\n",
            "|2012-01-17|59.87|60.11|59.52|59.85| 8500000| 52.20058|7.071764777688419...|\n",
            "|2012-01-18|59.79|60.03|59.65|60.01| 5911400| 52.34013|1.015495462653464...|\n",
            "|2012-01-19|59.93|60.73|59.75|60.61| 9234600|52.863445|  6.5763540967921E-6|\n",
            "|2012-01-20|60.75|61.25|60.67|61.01|10378800|53.212322| 5.90145296180676E-6|\n",
            "|2012-01-23|60.81|60.98|60.51|60.91| 7134100|53.125103|8.547679390846264E-6|\n",
            "|2012-01-24|60.75| 62.0|60.75|61.39| 7362800|53.543755|8.420709512685392E-6|\n",
            "|2012-01-25|61.18|61.61|61.04|61.47| 5915800| 53.61353|1.041448335142357...|\n",
            "|2012-01-26| 61.8|61.84|60.77|60.97| 7436200|53.177437|8.316075435382035E-6|\n",
            "|2012-01-27|60.86|61.12|60.54|60.71| 6287300|52.950665|9.721183804158345E-6|\n",
            "|2012-01-30|60.47|61.32|60.35| 61.3| 7636900|53.465256|8.029435987746889E-6|\n",
            "|2012-01-31|61.53|61.57|60.58|61.36| 9761500| 53.51759|6.307432228123159E-6|\n",
            "+----------+-----+-----+-----+-----+--------+---------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Determine la media de la columna Close"
      ],
      "metadata": {
        "id": "zplgUDe0oTbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Determine la media de la columna Close\n",
        "df_walmart.select(avg('Close').alias('promedio_close')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHbkjHhGskiS",
        "outputId": "92a0f943-dada-4f91-bda1-d0b1cde53dbc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|   promedio_close|\n",
            "+-----------------+\n",
            "|72.38844997363553|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Determine el máximo valor de High por año"
      ],
      "metadata": {
        "id": "z_q3FcMcofUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_walmart2.groupBy(year(col('Date')).alias('Año')).max('High').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0Qp-G8K-Cm2",
        "outputId": "47835014-6b25-477f-aea6-e0f8667c1ff0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "| Año|max(High)|\n",
            "+----+---------+\n",
            "|2015|    90.97|\n",
            "|2013|    81.37|\n",
            "|2014|    88.09|\n",
            "|2012|     77.6|\n",
            "|2016|    75.19|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Determine el promedio de Close por cada mes."
      ],
      "metadata": {
        "id": "ACgiU5Yioi2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_walmart2.groupBy(month(col('Date')).alias('Mes')).avg('Close').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3fOQLkLoiRR",
        "outputId": "1f2b8648-c672-41d4-fb98-81ca85bb40c1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+\n",
            "|Mes|       avg(Close)|\n",
            "+---+-----------------+\n",
            "| 12|72.84792482628012|\n",
            "|  1| 71.4480196131338|\n",
            "|  6| 72.4953774506191|\n",
            "|  3|71.77794376266337|\n",
            "|  5|72.30971685445533|\n",
            "|  9|72.18411782208611|\n",
            "|  4|72.97361900692894|\n",
            "|  8| 73.0298185521906|\n",
            "|  7|74.43971944078106|\n",
            "| 10| 71.5785454489968|\n",
            "| 11|72.11108927207418|\n",
            "|  2|71.30680438169499|\n",
            "+---+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 2 (70%)"
      ],
      "metadata": {
        "id": "dvVSZvWIy6yj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se tiene data de logs en JSON comprimidos que se almacenan cada una hora, se pretende enviar los logs de la hora anterior a la actual a una API, pero es necesario realizar un procesamiento de ellos primero. Los campos de los logs se definen de la siguiente manera:\n",
        "\n",
        "- `index_name`: correponde a un `string`el cual es un indexador del log y no puede ser `null`.\n",
        "- `detail`: es un diccionario tipo JSON, su estructura es dinámica y puede variar según el log. No puede ser `null`.\n",
        "- `timestamp`: corresponde al timestamp en unix epoch del instante en que fué generado el log. Debe ser un entero, pero no siempre llega así, tampoco puede ser `null`.\n",
        "- `group_by`: campo opcional, el cual corresponde a un string que sirve para agrupar el log.\n",
        "- `tags`: campo opcional, correponde a un array de strings para permitir hacer indexar los logs en la API y realizar queries más eficientes.\n",
        "\n",
        "Usted debe procesar los logs en un notebook de python con PySpark y escribir la data resultante de manera que cumpla con lo siguiente:\n",
        "\n",
        "- El archivo final debe contar con `index_name`, `detail`, `timestamp` y `tags`.\n",
        "- En caso de que exista `group_by` para un log, debe ser añadido al array de `tags`.\n",
        "- Si un log no tiene `tags` o es `null`, debe retornar un array vacío.\n",
        "- Asegurarse de que el campo `timestamp` sea de tipo entero. (Hint: cast y round)\n",
        "- Debe crear un schema para procesar la data, si bien con grandes volumenes de datos se infiere bien, para pocos logs puede no inferir correctamente.\n",
        "- Debe guardar la data procesada como archivo tipo JSON particionada en su Google Drive.\n",
        "- La data debe estar particionada por:\n",
        "  - `env`: corresponde al ambiente de ejecución (prod y qa)\n",
        "  - `index`: correpsonde al mismo valor de `index_name`.\n",
        "  - `group_by`: valor que ya viene en `group_by`, en caso de ser null, asignar valor: `NO_GROUP`\n",
        "  - `Year`, `Month`, `Day`, `Hour`, donde cada una de esas columnas corresponde a la hora de los logs en el path (hora anterior)\n",
        "\n",
        "Ejemplo ruta de salida:\n",
        "`drive/MyDrive/.../env=prod/index=dummy_index/group_by=NO_GROUP/year=2022/month=08/day=17/hour=21/filename.json`\n",
        "\n",
        "Ejemplo archivo de salida:\n",
        "```\n",
        "{\n",
        "  \"index_name\" : \"dummy_name\",\n",
        "  \"detail\" : {#some dummy dict}\n",
        "  \"timestamp\": 1231231321\n",
        "  \"tags\" : [\"tag_0\",....,\"tag_n\"]\n",
        "}\n",
        "```\n",
        "\n",
        "Consideraciones:\n",
        "- Asuma que la data que se encuentra en el path de Googe Drive es de la hora pasada a la actual.\n",
        "- En la creación de su Schema considere que algunos campos no siempre tendrán la misma estructura, para ello simplemente tratelos como `String`.\n",
        "- Para el ambiente, simplemente realice la ejecución 2 veces, una con `qa` y otra con `prod`.\n",
        "- Note que para particionar por fecha debe tomar la hora actual y restarle una hora (libería datetime en python)."
      ],
      "metadata": {
        "id": "zpzbCh9OzU-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Unir archivos en 1 dataframe\n",
        "Una vez unida la data podemos tener visualizacion, esto nos ayudará a saber si se debe hacer algun cambio"
      ],
      "metadata": {
        "id": "VNS9OR8OcHEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_path_order_data = '/content/tarea1.2'\n",
        "order_df = spark.read.json(input_path_order_data)"
      ],
      "metadata": {
        "id": "dplKAYBmcRi_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "order_df.printSchema() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Grr5JtW1eQwX",
        "outputId": "b11f17c3-c97a-4540-989a-e646a54a01a4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- detail: struct (nullable = true)\n",
            " |    |-- date: string (nullable = true)\n",
            " |    |-- item_uuid: string (nullable = true)\n",
            " |    |-- last_date_check: string (nullable = true)\n",
            " |    |-- number_of_products: long (nullable = true)\n",
            " |    |-- order_uuid: string (nullable = true)\n",
            " |    |-- shopper_id: long (nullable = true)\n",
            " |    |-- status: string (nullable = true)\n",
            " |    |-- store_id: long (nullable = true)\n",
            " |-- group_by: string (nullable = true)\n",
            " |-- index_name: string (nullable = true)\n",
            " |-- tags: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- timestamp: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Guardar data"
      ],
      "metadata": {
        "id": "qSEg23bd9efu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = '/content/tarea1.2/output_path'\n",
        "\n",
        "order_df.write.partitionBy('index_name').mode('append').csv(output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "AidXV3jdfyO-",
        "outputId": "deaef345-ebe4-41a2-dfa2-905ed1b22f19"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-167-fd37a327d9dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/tarea1.b/output_path'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0morder_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index_name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1025\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: CSV data source does not support struct<date:string,item_uuid:string,last_date_check:string,number_of_products:bigint,order_uuid:string,shopper_id:bigint,status:string,store_id:bigint> data type.;"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Viendo la cantidad de valores distintos tiene cada columna\n",
        "#order_df.select('group_by').distinct().show()\n",
        "order_df.select('index_name').distinct().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so2VK2ht5g-o",
        "outputId": "a3b8e627-ec61-4c77-f407-071325f41af2"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|         index_name|\n",
            "+-------------------+\n",
            "|     catalog_status|\n",
            "|             orders|\n",
            "|shoppers_dispatcher|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TAREA 2"
      ],
      "metadata": {
        "id": "2MnKnfnA94Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dog_food = spark.read.option('header', 'true').csv('/content/dog_food.csv')\n",
        "df_hack_data = spark.read.option('header', 'true').csv('/content/walmart_stock.csv')"
      ],
      "metadata": {
        "id": "-ZsxsQsj970H"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dog_food.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpXtZc_c972o",
        "outputId": "50650a9f-c6a5-4fa1-f817-2ddc5ab7d6fe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+----+---+-------+\n",
            "|  A|  B|   C|  D|Spoiled|\n",
            "+---+---+----+---+-------+\n",
            "|  4|  2|12.0|  3|    1.0|\n",
            "|  5|  6|12.0|  7|    1.0|\n",
            "|  6|  2|13.0|  6|    1.0|\n",
            "|  4|  2|12.0|  1|    1.0|\n",
            "|  4|  2|12.0|  3|    1.0|\n",
            "| 10|  3|13.0|  9|    1.0|\n",
            "|  8|  5|14.0|  5|    1.0|\n",
            "|  5|  8|12.0|  8|    1.0|\n",
            "|  6|  5|12.0|  9|    1.0|\n",
            "|  3|  3|12.0|  1|    1.0|\n",
            "|  9|  8|11.0|  3|    1.0|\n",
            "|  1| 10|12.0|  3|    1.0|\n",
            "|  1|  5|13.0| 10|    1.0|\n",
            "|  2| 10|12.0|  6|    1.0|\n",
            "|  1| 10|11.0|  4|    1.0|\n",
            "|  5|  3|12.0|  2|    1.0|\n",
            "|  4|  9|11.0|  8|    1.0|\n",
            "|  5|  1|11.0|  1|    1.0|\n",
            "|  4|  9|12.0| 10|    1.0|\n",
            "|  5|  8|10.0|  9|    1.0|\n",
            "+---+---+----+---+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_hack_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNPJVfL4975Y",
        "outputId": "4922fe4a-f9d2-4957-a834-f5240a04a2fd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
            "|      Date|              Open|              High|               Low|             Close|  Volume|         Adj Close|\n",
            "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
            "|2012-01-03|         59.970001|         61.060001|         59.869999|         60.330002|12668800|52.619234999999996|\n",
            "|2012-01-04|60.209998999999996|         60.349998|         59.470001|59.709998999999996| 9593300|         52.078475|\n",
            "|2012-01-05|         59.349998|         59.619999|         58.369999|         59.419998|12768200|         51.825539|\n",
            "|2012-01-06|         59.419998|         59.450001|         58.869999|              59.0| 8069400|          51.45922|\n",
            "|2012-01-09|         59.029999|         59.549999|         58.919998|             59.18| 6679300|51.616215000000004|\n",
            "|2012-01-10|             59.43|59.709998999999996|             58.98|59.040001000000004| 6907300|         51.494109|\n",
            "|2012-01-11|         59.060001|         59.529999|59.040001000000004|         59.400002| 6365600|         51.808098|\n",
            "|2012-01-12|59.790001000000004|              60.0|         59.400002|              59.5| 7236400|51.895315999999994|\n",
            "|2012-01-13|             59.18|59.610001000000004|59.009997999999996|59.540001000000004| 7729300|51.930203999999996|\n",
            "|2012-01-17|         59.869999|60.110001000000004|             59.52|         59.849998| 8500000|         52.200581|\n",
            "|2012-01-18|59.790001000000004|         60.029999|         59.650002|60.009997999999996| 5911400|         52.340131|\n",
            "|2012-01-19|             59.93|             60.73|             59.75|60.610001000000004| 9234600|         52.863447|\n",
            "|2012-01-20|             60.75|             61.25|         60.669998|61.009997999999996|10378800|53.212320999999996|\n",
            "|2012-01-23|         60.810001|             60.98|60.509997999999996|             60.91| 7134100|         53.125104|\n",
            "|2012-01-24|             60.75|              62.0|             60.75|61.389998999999996| 7362800| 53.54375400000001|\n",
            "|2012-01-25|             61.18|61.610001000000004|61.040001000000004|         61.470001| 5915800| 53.61353100000001|\n",
            "|2012-01-26|         61.799999|             61.84|             60.77|         60.970001| 7436200|         53.177436|\n",
            "|2012-01-27|60.860001000000004|         61.119999|60.540001000000004|60.709998999999996| 6287300|         52.950665|\n",
            "|2012-01-30|         60.470001|             61.32|         60.349998|         61.299999| 7636900|53.465256999999994|\n",
            "|2012-01-31|         61.529999|             61.57|         60.580002|61.360001000000004| 9761500|53.517590000000006|\n",
            "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## df_hack_data"
      ],
      "metadata": {
        "id": "2bG91zolG_iV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "metadata": {
        "id": "X9nbCLuQ978S"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labelIndex = StringIndexer(inputCol='Spoiled', outputCol='indexedLabel').fit(df_dog_food)"
      ],
      "metadata": {
        "id": "W9feFDyBLT0p"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dog_food.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0VjR8Q-LTn-",
        "outputId": "f0317955-1cbe-4831-ba59-062a5c86c786"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A', 'B', 'C', 'D', 'Spoiled']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "featureIndexer = VectorIndexer(inputCol = ['A', 'B', 'C', 'D'], outputCol= 'Spoiled', maxCategories=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "1RCepzMz5fOH",
        "outputId": "2e22238a-9da4-4960-8019-cfd7f06a6acc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-0ac7aa4dda49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatureIndexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'Spoiled'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxCategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'VectorIndexer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-zsYSPkw5-gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(trainingData, testData) = df_dog_food.randomSplit([0.7,0.3])"
      ],
      "metadata": {
        "id": "XHKJa747MIM_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(labelCol='indexedLabel', featuresCol='indexedFeatures', numTrees=10)"
      ],
      "metadata": {
        "id": "MX5-piF0LT8N"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labelConverter = IndexToString(inputCol='prediction', outputCol='predictedLabel', labels=labelIndex.labels)"
      ],
      "metadata": {
        "id": "29-HO10Ja5Fh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(stages=[labelIndex, featureIndexer, rf, labelConverter])"
      ],
      "metadata": {
        "id": "N70qJ28OcfSz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = pipeline.fit(trainingData)\n",
        "\n",
        "\n",
        "predictions = ModelRF.transform(testData)\n",
        "predictions.printSchema()\n",
        "predictions.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "eouNbwOsc5BD",
        "outputId": "4a320a28-9996-4b69-a51b-7b973d8d2858"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-f925b59b8f46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelRF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ModelRF' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##df_hack_data"
      ],
      "metadata": {
        "id": "FvZIsfLyHImv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aQ17IArT97-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "LHmEKUbTw_Hp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}